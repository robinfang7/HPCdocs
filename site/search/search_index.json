{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HPC\u5c0e\u89bd\u624b\u518a","text":"<p>\u672c\u6587\u4ef6\u63d0\u4f9b\u7528\u6236\u5b78\u7fd2\u4f7f\u7528NCHC\u7684\u8d85\u7d1a\u96fb\u8166\uff0c\u5efa\u8b70\u521d\u5b78\u8005\u5148\u5f9e\u5165\u9580\u6307\u5357\u700f\u89bd\uff0c\u60f3\u4e86\u89e3NCHC\u5404\u9805\u8d85\u7d1a\u96fb\u8166\u8acb\u81f3\u8d85\u7d1a\u96fb\u8166\uff0c\u5e33\u865f\u8207\u8a08\u5283\u7ba1\u7406\u8acb\u81f3iService\uff0c\u57f7\u884c\u76f8\u95dc\u9818\u57df\u7684\u61c9\u7528\u7a0b\u5f0f\uff0c\u8acb\u81f3\u61c9\u7528\u7a0b\u5f0f\u3002</p>"},{"location":"#_1","title":"\ud83c\udfaf\u5165\u9580\u6307\u5357","text":""},{"location":"#_2","title":"\ud83d\udda5\ufe0f\u8d85\u7d1a\u96fb\u8166","text":""},{"location":"#iservice","title":"\ud83e\udd16iService","text":""},{"location":"#_3","title":"\ud83d\udcf1\u61c9\u7528\u7a0b\u5f0f","text":""},{"location":"#_4","title":"\ud83e\udd14\u5e38\u898b\u554f\u984c","text":"<ul> <li>\u5c0diService\u5e33\u865f\u8207\u8a08\u756b\u6709\u554f\u984c\uff0c\u8acb\u81f3\u5ba2\u670d\u4fe1\u7bb1 iservice@niar.org.tw</li> <li>\u5c0dNCHC\u8d85\u7d1a\u96fb\u8166\u6709\u4f7f\u7528\u554f\u984c\uff0c\u8acb\u81f3\u6280\u8853\u4fe1\u7bb1 isupport@niar.org.tw</li> <li>iService\u5ba2\u670d\u5c08\u7dda\uff1a0809-091-365 (24\u5c0f\u6642\u5ba2\u670d\u5c08\u7dda)</li> <li>\u5c0d\u672c\u7db2\u7ad9\u5167\u5bb9\u6709\u610f\u898b\u8acb\u81f3Github issue\u7559\u8a00</li> </ul>"},{"location":"getting-started/","title":"\ud83c\udfaf\u5165\u9580\u6307\u5357","text":""},{"location":"getting-started/#_1","title":"\u5165\u9580\u6307\u5357","text":"<ol> <li> <p>\u8a3b\u518a\u5e33\u865f     a. \u9ede\u9078iService\u7db2\u9801     b. \u9ede\u9078\u8a3b\u518a     c. \u4ee5\u96fb\u5b50\u90f5\u4ef6\u4f5c\u70ba\u6703\u54e1\u5e33\u865f     d. \u586b\u5beb\u6703\u54e1\u8cc7\u6599\u8207\u4e3b\u6a5f\u5e33\u865f      e. \u6536\u53d6 e-mail \u8a8d\u8b49\u4fe1\u7bb1\u3001\u6536\u53d6\u7c21\u8a0a\u8a8d\u8b49\u624b\u6a5f\u865f\u78bc\uff0c\u5373\u5b8c\u6210\u8a3b\u518a  </p> <p>Note</p> <ul> <li>\u8a3b\u518a\u5e33\u865f\u7684\u8aaa\u660e1 </li> <li>\u8a3b\u518a\u5e33\u865f\u7684\u8aaa\u660e2</li> </ul> </li> <li> <p>\u8a2d\u5b9aOTP\u8f09\u5177 </p> <p>NCHC\u7684\u8d85\u7d1a\u96fb\u8166\u7684\u767b\u5165\u65b9\u5f0f\u63a1\u7528\u4e8c\u968e\u6bb5\u8a8d\u8b49\uff0c\u7b2c\u4e00\u500b\u662f\u4e3b\u6a5f\u5e33\u865f\u7684\u5bc6\u78bc\uff0c\u7b2c\u4e8c\u500b\u662fOTP(One Time Password)\u3002  </p> </li> <li> <p>\u767b\u5165\u4e3b\u6a5f\uff0c\u9032\u884c\u5e73\u884c\u904b\u7b97     a. \u8a08\u756b\u4ee3\u865f\uff0c\u82e5\u7121\u8a08\u756b\uff0c\u53c3\u95b1\u7533\u8acb\u8a08\u756b     b. \u6e96\u5099SSH\u9023\u7dda\u8edf\u9ad4 (MobaXterm, Putty for Windows, Terminal(\u7d42\u7aef\u6a5f) for Linux/MacOS)  </p> <p>\u6848\u4f8b1\uff1a\u7528MobaXterm\u767b\u5165\u5275\u9032\u4e00\u865f\uff0c\u57f7\u884c\u5e73\u884c\u904b\u7b97\u7a0b\u5f0f\uff0c\u56de\u50b3\u7bc0\u9ede\u548c\u8655\u7406\u6838\u5fc3\u7684\u5e8f\u865f(Rank)     (1) \u958b\u555fMobaXterm\uff0c\u9ede\u9078Start local terminal        (2) \u8f38\u5165ssh\u6307\u4ee4 <code>ssh &lt;account&gt;@f1-ilgn01.nchc.org.tw</code>     (3) \u9078\u64c7OPT\u65b9\u5f0f\uff0c\u9019\u88e1\u90782\uff0c\u7528\u624b\u6a5f\u57f7\u884cIDExpert\u7684\u63a8\u64ad\u3002\u8f38\u5165\u4e3b\u6a5f\u5e33\u865f\u7684\u5bc6\u78bc\u3002      (4) \u5728\u624b\u6a5f\u4e0a\u9ede\u9078\u63a8\u64ad      (5) \u5b8c\u6210\u767b\u5165\u5275\u9032\u4e00\u865f\u7bc0\u9ede      (6) \u7de8\u8f2f<code>mpi_test.c</code> <pre><code>#define _GNU_SOURCE  // \u5fc5\u9808\u5728\u6240\u6709 include \u4e4b\u524d\u5b9a\u7fa9\uff0c\u4ee5\u4f7f\u7528 sched_getcpu\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;sched.h&gt;   // \u7528\u65bc sched_getcpu()\n\nint main(int argc, char** argv) {\n    // \u521d\u59cb\u5316 MPI \u74b0\u5883\n    MPI_Init(&amp;argc, &amp;argv);\n\n    // \u53d6\u5f97\u7e3d\u9032\u7a0b\u6578 (Size) \u8207 \u7576\u524d\u9032\u7a0b\u7de8\u865f (Rank)\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    // \u53d6\u5f97\u7bc0\u9ede\u4e3b\u6a5f\u540d\u7a31 (Hostname)\n    char hostname[256];\n    gethostname(hostname, sizeof(hostname));\n\n    // \u53d6\u5f97\u7576\u524d\u904b\u4f5c\u7684 CPU \u6838\u5fc3\u7de8\u865f\n    // \u6ce8\u610f\uff1a\u9019\u662f\u4f5c\u696d\u7cfb\u7d71\u5c64\u7d1a\u7684 CPU ID (\u908f\u8f2f\u6838\u5fc3)\n    int cpu_id = sched_getcpu();\n\n    // \u8f38\u51fa\u8cc7\u8a0a\n    // \u70ba\u4e86\u907f\u514d\u8f38\u51fa\u6df7\u4e82\uff0c\u901a\u5e38\u53ea\u6709 Rank 0 \u6216\u8005\u5404\u81ea print \u4f46\u9806\u5e8f\u4e0d\u4fdd\u8b49\n    // \u9019\u88e1\u8b93\u6bcf\u500b Rank \u90fd\u8f38\u51fa\uff0c\u4ee5\u4fbf\u89c0\u5bdf\u5206\u4f48\n    printf(\"MPI Rank: %03d | Host: %s | CPU_ID: %03d\\n\",\n          world_rank, hostname, cpu_id);\n\n    // \u7d50\u675f MPI \u74b0\u5883\n    MPI_Finalize();\n\n    return 0;\n} \n</code></pre>     (7) \u7de8\u8f2fslurm\u8173\u672c <code>run_mpi.slurm</code>\uff0c\u7d22\u53d6\u5169\u500b\u7bc0\u9ede\uff0c\u6bcf\u500b\u7bc0\u9ede\u6709\u5169\u500bCPU\u6838\u5fc3\u3002\u555f\u7528Inte mpi compiler\uff0c\u7de8\u8b6fmpi_test.c\u4e26\u8f38\u51fampi_test\u57f7\u884c\u6a94\uff0c\u57f7\u884cmpi_test\u57f7\u884c\u6a94\u7684\u5e73\u884c\u904b\u7b97\u3002 <pre><code>#!/bin/bash\n#SBATCH --account=&lt;ProjectID&gt;      # (-A) iService Project ID\n#SBATCH --job-name=mpi_test        # (-J) Job name\n#SBATCH --partition=development    # (-p) Slurm partition\n#SBATCH --nodes=2                  # (-N) Maximum number of nodes to be allocated\n#SBATCH --cpus-per-task=1          # (-c) Number of cores per MPI task\n#SBATCH --ntasks-per-node=2        # Maximum number of tasks on each node\n#SBATCH --time=00:30:00            # (-t) Wall time limit (days-hrs:min:sec)\n#SBATCH --output=job-%j.out        # (-o) Path to the standard output file\n\nmodule purge\nmodule load intel/2024_01_46 # enable intel mpi compilier\n\necho \"Compiling code...\"\nmpicc -o mpi_test mpi_test.c\n\necho -e \"\\nExecuting mpi_test...\\n\"\nmpirun -np 4 ./mpi_test\necho -e \"\\nJob finished.\"\n</code></pre>     (8) \u57f7\u884c\u8173\u672c<code>sbatch run_mpi.slurm</code>\uff0c\u7522\u751fJob ID\uff0c\u89c0\u5bdf\u8f38\u51fa\u7d50\u679c\u3002  <pre><code>[&lt;account&gt;@ilgn01 mpi_test]$ sbatch run_mpi.slurm\nSubmitted batch job 565924\n[&lt;account&gt;@ilgn01 mpi_test]$ cat *565924*\nCompiling code...\n\nExecuting mpi_test...\n\nMPI Rank: 000 | Host: icpnp152 | CPU_ID: 018\nMPI Rank: 001 | Host: icpnp152 | CPU_ID: 019\nMPI Rank: 002 | Host: icpnp153 | CPU_ID: 045\nMPI Rank: 003 | Host: icpnp153 | CPU_ID: 101\n\nJob finished.\n</code></pre></p> <p>\u6848\u4f8b2\uff1a\u7528MobaXterm\u767b\u5165\u6676\u527525\uff0c\u57f7\u884cPytorh\u5e73\u884c\u7a0b\u5f0f\uff0c\u56de\u50b3\u7bc0\u9ede\u548c\u8655\u7406\u6838\u5fc3\u7684\u5e8f\u865f(Rank)      (1) \u958b\u555fMobaXterm\uff0c\u8f38\u5165ssh\u6307\u4ee4 <code>ssh &lt;account&gt;@nano5.nchc.org.tw</code>     (2) \u7de8\u8f2fPytorch\u7a0b\u5f0f<code>train_multi_gpu.py</code> <pre><code>import os\nimport torch\nimport torch.distributed as dist\nimport argparse\n\ndef main():\n    # 1. \u521d\u59cb\u5316\u5206\u4f48\u5f0f\u74b0\u5883\n    # PyTorch \u5efa\u8b70\u4f7f\u7528 'env://' \u521d\u59cb\u5316\uff0c\u5b83\u6703\u5f9e\u74b0\u5883\u8b8a\u6578 (\u5982 Slurm \u8a2d\u7f6e\u7684) \u81ea\u52d5\u7372\u53d6\u6240\u9700\u8cc7\u8a0a\u3002\n    dist.init_process_group(backend=\"nccl\")\n\n    # 2. \u7372\u53d6\u4e26\u8f38\u51fa\u95dc\u9375\u7684\u5206\u4f48\u5f0f\u8cc7\u8a0a\n\n    # \u7e3d\u9ad4 (Global) Rank: \u7a0b\u5f0f\u5728\u6240\u6709\u7bc0\u9ede/\u6240\u6709 GPU \u4e2d\u7684\u552f\u4e00 ID (\u5f9e 0 \u5230 N-1)\n    global_rank = dist.get_rank()\n\n    # \u4e16\u754c\u5927\u5c0f (World Size): \u53c3\u8207\u8a13\u7df4\u7684\u7e3d\u9032\u7a0b/GPU \u6578\u91cf\n    world_size = dist.get_world_size()\n\n    # \u7bc0\u9ede (Local) Rank: \u7a0b\u5f0f\u5728\u7576\u524d\u7bc0\u9ede (\u4e3b\u6a5f) \u5167\u7684\u552f\u4e00 ID (\u5f9e 0 \u5230 G-1, G \u70ba\u8a72\u7bc0\u9ede GPU \u6578)\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n\n    # \u7576\u524d\u7bc0\u9ede\u7684\u540d\u7a31 (Hostname)\n    node_name = os.environ.get(\"SLURMD_NODENAME\", \"Unknown Node\")\n\n    # \u7372\u53d6\u4e26\u8a2d\u7f6e\u7576\u524d\u9032\u7a0b\u61c9\u8a72\u4f7f\u7528\u7684 GPU \u8a2d\u5099\n    if torch.cuda.is_available():\n        # \u8a2d\u7f6e\u8a2d\u5099\u70ba\u8a72\u9032\u7a0b\u7684 local_rank \u5c0d\u61c9\u7684 GPU\n        # PyTorch DDP \u548c Slurm/Torchrun \u6703\u78ba\u4fdd local_rank \u6b63\u78ba\u5c0d\u61c9\u5230\u53ef\u7528\u7684 GPU\n        device = torch.device(f\"cuda:{local_rank}\")\n        torch.cuda.set_device(device)\n        gpu_name = torch.cuda.get_device_name(device)\n    else:\n        device = torch.device(\"cpu\")\n        gpu_name = \"N/A\"\n\n    # \u8f38\u51fa\u6240\u9700\u8cc7\u8a0a\n    print(f\"--- Process Info ---\")\n    print(f\"Node Name: {node_name}\")\n    print(f\"Global Rank: {global_rank} / {world_size}\")\n    print(f\"Local Rank (GPU Index on Node): {local_rank}\")\n    print(f\"Using Device: {device} ({gpu_name})\")\n\n    # 3. \u6a21\u64ec\u8a13\u7df4\u6b65\u9a5f (\u53ef\u66ff\u63db\u70ba\u60a8\u7684\u5be6\u969b\u6a21\u578b\u548c\u6578\u64da\u96c6)\n    if global_rank == 0:\n        print(\"\\nStarting a mock training loop...\")\n\n    # \u5275\u5efa\u4e00\u500b\u5c0f\u578b\u6a21\u578b\u4e26\u5c07\u5176\u79fb\u52d5\u5230\u6b63\u78ba\u7684\u8a2d\u5099\n    model = torch.nn.Linear(10, 1).to(device)\n    # \u5c07\u6a21\u578b\u5305\u88f9\u5728 DDP \u4e2d\n    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None)\n\n    # \u6a21\u64ec\u6578\u64da\u548c\u8a13\u7df4\u6b65\u9a5f\n    data = torch.randn(2, 10).to(device)\n    target = torch.randn(2, 1).to(device)\n    loss_fn = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.01)\n\n    # \u6a21\u64ec\u4e00\u500b\u8a13\u7df4\u6b65\u9a5f\n    output = ddp_model(data)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n\n    # \u78ba\u4fdd\u6240\u6709\u9032\u7a0b\u90fd\u5b8c\u6210\u5f8c\u518d\u9000\u51fa\n    dist.barrier()\n\n    if global_rank == 0:\n        print(\"Mock training complete. All processes synchronized.\")\n\n    # 4. \u6e05\u7406\u5206\u4f48\u5f0f\u74b0\u5883\n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>     (3) \u5230NVIDIA NGC Catalog\u7db2\u9801\uff0c\u7528Singularity\u6307\u4ee4\u4e0b\u8f09Pytorch\u5bb9\u5668\u6620\u50cf\u6a94 <pre><code>singularity pull pytorch_24.08-py3.sif docker://nvcr.io/nvidia/pytorch:24.08-py3\n</code></pre>     (4) \u7de8\u8f2fslurm\u8173\u672c<code>run_ddp.slurm</code>\uff0c\u7d22\u53d6\u5169\u500b\u7bc0\u9ede\uff0c\u6bcf\u500b\u7bc0\u9ede\u6709\u5169\u500bGPU\uff0c\u5f15\u7528pytorch\u5bb9\u5668 <pre><code>#!/bin/bash\n#SBATCH -A &lt;Project_ID&gt;           # iService Project id\n#SBATCH -J ddp_test                # job name\n#SBATCH -p normal                # partition dev normal normal2\n#SBATCH --nodes=2              # Maximum number of nodes to be allocated\n#SBATCH --cpus-per-task=4      # Number of cores per MPI task\n#SBATCH --ntasks-per-node=1    # Number of MPI tasks (i.e. processes)\n#SBATCH --gres=gpu:2\n#SBATCH -o %x_%j.out          # Path to the standard output file\n\nexport OMP_NUM_THREADS=1\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nexport MASTER_PORT=29500\necho \"Starting distributed job with MASTER_ADDR=$MASTER_ADDR and MASTER_PORT=$MASTER_PORT\"\n\n# get GPU count\nIFS=',' read -ra gpu_array &lt;&lt;&lt; \"$SLURM_JOB_GPUS\"\nexport GPUS_PER_NODE=${#gpu_array[@]}\n\nSIF=/path/to/pytorch_24.08-py3.sif\nSINGULARITY=\"singularity run -B /work:/work --nv $SIF\"\n\nCMD=\"torchrun \\\n --nnodes $SLURM_NNODES \\\n --nproc_per_node $GPUS_PER_NODE \\\n --rdzv_id $SLURM_JOB_ID \\\n --rdzv_backend c10d \\\n --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \\\n train_multi_gpu.py\"\n\nRUN=\"srun --ntasks=$GPUS_PER_NODE --nodes=$SLURM_NNODES $SINGULARITY $CMD\"\necho \"$RUN\"; $RUN\necho -e \"\\nJob finished.\\n\"\n</code></pre>     (5) \u57f7\u884c\u8173\u672c<code>sbatch run_ddp.slurm</code>\uff0c\u7522\u751fJob ID\uff0c\u89c0\u5bdf\u8f38\u51fa\u7d50\u679c\u3002 <pre><code>[&lt;account&gt;@cbi-lgn01 pytorch_test]$ sbatch run_ddp.slurm\nSubmitted batch job 78728\n[&lt;account&gt;@cbi-lgn01 pytorch_test]$ cat *78728*\nStarting distributed job with MASTER_ADDR=hgpn02 and MASTER_PORT=29500\nsrun --ntasks=2 --nodes=2 singularity run -B /work:/work --nv /work/u8880716/sif/pytorch_24.08-py3.sif torchrun  --nnodes 2  --nproc_per_node 2  --rdzv_id 78728  --rdzv_backend c10d  --rdzv_endpoint hgpn02:29500  train_multi_gpu.py\n\n....\u5bb9\u5668\u555f\u52d5\u8a0a\u606f...\n\n--- Process Info ---\nNode Name: hgpn02\nGlobal Rank: 0 / 4\nLocal Rank (GPU Index on Node): 0\nUsing Device: cuda:0 (NVIDIA H100 80GB HBM3)\n\nStarting a mock training loop...\n--- Process Info ---\nNode Name: hgpn03\nGlobal Rank: 2 / 4\nLocal Rank (GPU Index on Node): 0\nUsing Device: cuda:0 (NVIDIA H100 80GB HBM3)\n--- Process Info ---\nNode Name: hgpn02\nGlobal Rank: 1 / 4\nLocal Rank (GPU Index on Node): 1\nUsing Device: cuda:1 (NVIDIA H100 80GB HBM3)\n--- Process Info ---\nNode Name: hgpn03\nGlobal Rank: 3 / 4\nLocal Rank (GPU Index on Node): 1\nUsing Device: cuda:1 (NVIDIA H100 80GB HBM3)\nMock training complete. All processes synchronized.\n\nJob finished.\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#_2","title":"\u6df1\u5165\u4e86\u89e3","text":""},{"location":"iservice/","title":"iService","text":""},{"location":"iservice/#_1","title":"\u8a08\u756b\u7ba1\u7406\u8005\u5177\u6709\u4ee5\u4e0b\u529f\u80fd\u7684\u6b0a\u9650","text":"<ul> <li> <p>\u7533\u8acb\u8a08\u756b</p> <ol> <li>\u9ede\u9078\u6703\u54e1\u4e2d\u5fc3&gt;\u8a08\u5283\u7ba1\u7406&gt;\u6211\u7684\u8a08\u756b </li> <li> <p>\u9ede\u9078\u65b0\u589e </p> </li> <li> <p>\u570b\u79d1\u6703\u8a08\u756b</p> </li> <li>\u5b78\u8853\u8a08\u756b</li> <li>\u653f\u5e9c\u8207\u6cd5\u4eba\u8a08\u756b</li> <li>\u4f01\u696d\u8207\u500b\u4eba\u8a08\u756b</li> </ol> </li> <li> <p>\u52a0\u5165\u8a08\u756b\u6210\u54e1</p> </li> <li>\u8a2d\u5b9a\u6210\u54e1\u7684\u5b50\u9322\u5305</li> <li>\u8a2d\u5b9a\u6210\u54e1\u70ba\u7ba1\u7406\u54e1</li> <li>HFS\u5bb9\u91cf\u8a2d\u5b9a</li> </ul>"},{"location":"supercomputer/","title":"\u8d85\u7d1a\u96fb\u8166","text":"<p>\u9ad8\u901f\u8a08\u7b97\u662f\u79d1\u5b78\u7814\u767c\u7684\u57fa\u790e\uff0c\u8eab\u70ba\u5168\u570b\u552f\u4e00\u63d0\u4f9b\u79d1\u5b78\u3001\u7814\u767c\u7b49\u8de8\u9818\u57df\u5c08\u9577\u6240\u9700\u4e4b\u9ad8\u901f\u8a08\u7b97\u670d\u52d9\u7684\u570b\u5bb6\u5be6\u9a57\u5ba4\uff0c\u570b\u7db2\u4e2d\u5fc3\u898f\u5283\u5efa\u7f6e\u5168\u7403\u5148\u9032\u7684\u512a\u8cea\u8a08\u7b97\u8207\u8cc7\u6599\u5132\u5b58\u74b0\u5883\u3001\u9ad8\u6548\u80fd\u500b\u4eba\u96fb\u8166\u53e2\u96c6\uff0c\u4ee5\u53ca\u5404\u7a2e\u5de5\u7a0b\u8207\u79d1\u5b78\u61c9\u7528\u8edf\u9ad4\u53ca\u8cc7\u6599\u5eab\uff0c\u4ee5\u63d0\u4f9b\u7522\u5b78\u7814\u5404\u754c\u4f7f\u7528\u670d\u52d9\uff0c\u9032\u800c\u4fc3\u4f7f\u7528\u6236\u5728\u5b78\u8853\u7684\u7522\u51fa\u6709\u9ad8\u7e3e\u6548\u7684\u8ca2\u737b\u3002</p> <p>\u4ee5\u4e0b\u70ba\u570b\u7db2\u4e2d\u5fc3\u670d\u52d9\u4e2d\u8d85\u7d1a\u96fb\u8166\uff1a</p>"},{"location":"supercomputer/#25","title":"\u6676\u527525","text":"<p>\u6676\u527525\uff08Nano 5\uff09,  \u6676\u527525\u4f7f\u7528\u624b\u518a,  Queue \u8207\u8a08\u7b97\u8cc7\u6e90\u4f7f\u7528\u8aaa\u660e</p>"},{"location":"supercomputer/#_2","title":"\u5275\u9032\u4e00\u865f","text":"<p>\u5275\u9032\u4e00\u865f\uff08Forerunner 1\uff09,  \u5275\u9032\u4e00\u865f\u4f7f\u7528\u624b\u518a,  Queue \u8207\u8a08\u7b97\u8cc7\u6e90\u4f7f\u7528\u8aaa\u660e</p>"},{"location":"supercomputer/#_3","title":"\u53f0\u7063\u6749\u4e09\u865f","text":"<p>\u53f0\u7063\u6749\u4e09\u865f (TAIWANIA 3),  \u4f7f\u7528\u624b\u518a,  Queue \u8207\u8a08\u7b97\u8cc7\u6e90\u4f7f\u7528\u8aaa\u660e</p>"},{"location":"supercomputer/#_4","title":"\u53f0\u7063\u6749\u4e8c\u865f","text":"<p>\u53f0\u7063\u6749\u4e8c\u865f (TAIWANIA 2),  \u53f0\u7063\u6749\u4e8c\u865f\u4f7f\u7528\u624b\u518a</p>"},{"location":"supercomputer/#_5","title":"\u5404\u9805\u8d85\u7d1a\u96fb\u8166\u5f59\u6574","text":"\u6676\u527525 \u5275\u9032\u4e00\u865f \u53f0\u7063\u6749\u4e09\u865f \u53f0\u7063\u6749\u4e8c\u865f \u898f\u683c 111 222 333 444 <p>\u4e86\u89e3\u66f4\u591a\u570b\u7db2\u4e2d\u5fc3\u6b77\u4ee3\u8d85\u7d1a\u96fb\u8166\u570b\u7db2\u4e2d\u5fc3\u8d85\u7d1a\u96fb\u8166\u767c\u5c55\u6b77\u7a0bPDF\u4e0b\u8f09</p>"},{"location":"supercomputer/#_6","title":"\u8d85\u7d1a\u96fb\u8166\u7684\u8edf\u9ad4\u7ba1\u7406","text":""},{"location":"supercomputer/#slurm","title":"Slurm\u5de5\u4f5c\u6392\u7a0b\u7cfb\u7d71","text":"<ul> <li>Slurm\u5de5\u4f5c\u6392\u7a0b\u7cfb\u7d71: apps/management/slurm.md</li> <li>\u8cc7\u6e90\u67e5\u8a62</li> </ul>"},{"location":"supercomputer/#lmod","title":"Lmod\u8edf\u9ad4\u8edf\u9ad4\u6a21\u7d44","text":"<ul> <li>Lmod\u6a21\u7d44\u7cfb\u7d71: apps/management/lmod.md</li> </ul>"},{"location":"supercomputer/#singularity","title":"Singularity\u5bb9\u5668\u7ba1\u7406","text":""},{"location":"getting-started/dive-deeper/","title":"\u8cc7\u6599\u50b3\u8f38","text":"<p>\u6676\u527525 \u5275\u9032\u4e00\u865f \u53f0\u7063\u6749\u4e09\u865f \u53f0\u7063\u6749\u4e8c\u865f </p>"},{"location":"getting-started/dive-deeper/#_2","title":"\u5716\u5f62\u4ecb\u9762\u767b\u5165","text":"<p>\u6676\u527525 \u5275\u9032\u4e00\u865f \u53f0\u7063\u6749\u4e09\u865f </p>"},{"location":"getting-started/dive-deeper/#open-ondemand","title":"Open OnDemand","text":"<p>\u5275\u9032\u4e00\u865f:\u7279\u6b8a\u5de5\u4f5c\u7bc0\u9ede\u64cd\u4f5c\u6d41\u7a0b </p>"},{"location":"getting-started/dive-deeper/#_3","title":"\u67e5\u8a62\u8cc7\u6e90","text":"<p>\u67e5\u8a62\u7bc0\u9ede\u72c0\u614b <code>sinfo</code> \u67e5\u8a62\u7bc0\u9ede\u6709\u7a7a\u7684GPU <pre><code>scontrol show node | grep -E 'NodeName|AllocTRES' | sed -n '/NodeName/{N; /gres\\/gpu=8/!p}'\n</code></pre></p>"},{"location":"getting-started/dive-deeper/#_4","title":"\u5bb9\u5668\u6253\u5305","text":"<p>Singularity...</p>"}]}